#!/usr/bin/env python3
"""
Base agent for LLM-powered game agents.

This module provides a BaseAgent class that defines the common interface
and functionality for all agent types (Narrator, RuleChecker, ContextEvaluator).
"""

import os
import json
import datetime
from typing import Dict, List, Optional, Any, Tuple, Union
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass

from core.utils.logging_config import get_logger
from core.base.config import get_config
from core.llm.llm_manager import LLMManager, LLMResponse, LLMRole, get_llm_manager
from core.llm.provider_manager import ProviderType

# Get the module logger
logger = get_logger("AGENT")

@dataclass
class AgentContext:
    """
    Context for an agent.
    
    This dataclass contains the context information provided to an agent
    for processing a request, including game state, player input, and
    relevant memories.
    """
    # Game state information
    game_state: Dict[str, Any]
    player_state: Dict[str, Any]
    world_state: Dict[str, Any]
    
    # Input and history
    player_input: str
    conversation_history: List[Dict[str, Any]]
    
    # Memory context (if applicable)
    relevant_memories: List[Dict[str, Any]] = None
    
    # Additional context
    context_summary: Optional[str] = None
    additional_context: Optional[Dict[str, Any]] = None


@dataclass
class AgentResponse:
    """
    Response from an agent.
    
    This dataclass contains the response generated by an agent,
    including the output text, any extracted commands, and metadata.
    """
    # Output text
    content: str
    
    # Extracted LLM commands (if any)
    commands: List[Tuple[str, str]] = None
    
    # Metadata
    metadata: Dict[str, Any] = None
    
    # LLM response details
    llm_response: Optional[LLMResponse] = None
    
    # Timestamp
    timestamp: str = None
    
    def __post_init__(self):
        """Initialize default values."""
        if self.commands is None:
            self.commands = []
        
        if self.metadata is None:
            self.metadata = {}
        
        if self.timestamp is None:
            self.timestamp = datetime.datetime.now().isoformat()


class BaseAgent(ABC):
    """
    Base class for all agents.
    
    This abstract class defines the common interface and functionality
    for all agent types (Narrator, RuleChecker, ContextEvaluator).
    """
    
    def __init__(self, agent_name: str, agent_id: Optional[str] = None):
        """
        Initialize the base agent.
        
        Args:
            agent_name: The name of the agent.
            agent_id: Optional unique identifier for the agent.
        """
        self.agent_name = agent_name
        self.agent_id = agent_id or f"{agent_name.lower()}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Get configuration
        self._config = get_config()
        
        # Get LLM manager
        self._llm_manager = get_llm_manager()
        
        # Load agent settings
        self._settings = self._load_agent_settings()
        
        # Set provider settings
        self._provider_type = self._get_provider_type()
        self._model = self._settings.get("model", None)
        self._temperature = self._settings.get("temperature", 0.7)
        
        logger.info(f"Initialized {self.agent_name} agent (ID: {self.agent_id})")
    
    def _load_agent_settings(self) -> Dict[str, Any]:
        """
        Load agent settings from configuration.
        
        Returns:
            Dictionary of agent settings.
        """
        # Default settings
        default_settings = {
            "provider_type": None,  # Use system default
            "model": None,  # Use provider default
            "temperature": 0.7,
            "max_tokens": 1000,
            "timeout_seconds": 30,
            "system_prompt_template": "You are {agent_name}, a helpful assistant for an RPG game.",
            "include_conversation_history": True,
            "include_memories": True,
            "max_conversation_entries": 10,
            "max_memory_entries": 5
        }
        
        # Try to load agent-specific settings
        agent_settings_path = os.path.join("config", "llm", "agents", f"{self.agent_name.lower()}.json")
        
        if os.path.exists(agent_settings_path):
            try:
                with open(agent_settings_path, 'r', encoding='utf-8') as f:
                    agent_settings = json.load(f)
                
                # Merge with default settings
                merged_settings = {**default_settings, **agent_settings}
                logger.info(f"Loaded settings for {self.agent_name} agent from {agent_settings_path}")
                return merged_settings
            
            except Exception as e:
                logger.error(f"Error loading agent settings: {e}")
                logger.info(f"Using default settings for {self.agent_name} agent")
                return default_settings
        
        # If no agent-specific settings file exists, create one with default settings
        try:
            os.makedirs(os.path.dirname(agent_settings_path), exist_ok=True)
            with open(agent_settings_path, 'w', encoding='utf-8') as f:
                json.dump(default_settings, f, indent=4)
            logger.info(f"Created default settings file for {self.agent_name} agent at {agent_settings_path}")
        except Exception as e:
            logger.error(f"Error creating default agent settings file: {e}")
        
        return default_settings
    
    def _get_provider_type(self) -> ProviderType:
        """
        Get the provider type for this agent.
        
        Returns:
            The provider type enum value.
        """
        provider_type_str = self._settings.get("provider_type")
        
        if provider_type_str and provider_type_str != "None":
            try:
                return ProviderType[provider_type_str]
            except (KeyError, ValueError):
                logger.warning(f"Invalid provider type: {provider_type_str}. Using system default.")
        
        # Use system default
        default_provider = self._llm_manager._provider_manager.get_default_provider()
        if not default_provider:
            logger.error(f"No default provider available for {self.agent_name} agent")
            # Fall back to OPENAI if we have no default
            return ProviderType.OPENAI
            
        return default_provider
    
    def _generate_system_prompt(self, context: AgentContext) -> str:
        """
        Generate the system prompt for the agent.
        
        This method should be overridden by subclasses to provide
        agent-specific system prompts.
        
        Args:
            context: The agent context.
        
        Returns:
            The system prompt string.
        """
        # Get the template
        template = self._settings.get("system_prompt_template", 
                                     "You are {agent_name}, a helpful assistant for an RPG game.")
        
        # Format with agent name
        system_prompt = template.format(agent_name=self.agent_name)
        
        return system_prompt
    
    def _format_conversation_history(self, context: AgentContext) -> List[Dict[str, str]]:
        """
        Format conversation history for the LLM prompt.
        
        Args:
            context: The agent context.
        
        Returns:
            List of message dictionaries (role, content).
        """
        messages = []
        
        if not self._settings.get("include_conversation_history", True):
            return messages
        
        # Get the max number of entries to include
        max_entries = self._settings.get("max_conversation_entries", 10)
        
        # Get the most recent entries
        recent_history = context.conversation_history[-max_entries:] if context.conversation_history else []
        
        # Format each entry as a message
        for entry in recent_history:
            role = entry.get("role", "user")
            content = entry.get("content", "")
            
            # Map game roles to LLM roles
            if role == "player":
                messages.append({"role": "user", "content": content})
            elif role == "gm":
                messages.append({"role": "assistant", "content": content})
            elif role == "system":
                # Skip system messages in conversation history
                continue
            else:
                # For unknown roles, use user role
                messages.append({"role": "user", "content": f"[{role}] {content}"})
        
        return messages
    
    def _format_memories(self, context: AgentContext) -> str:
        """
        Format memory context for the LLM prompt.
        
        Args:
            context: The agent context.
        
        Returns:
            Formatted memory context string.
        """
        if not context.relevant_memories or not self._settings.get("include_memories", True):
            return ""
        
        memory_lines = ["### Relevant Memories:"]
        
        # Get the max number of entries to include
        max_entries = self._settings.get("max_memory_entries", 5)
        
        # Format each memory entry
        for i, memory in enumerate(context.relevant_memories[:max_entries]):
            # Extract memory details
            content = memory.get("content", "")
            importance = memory.get("importance", 0)
            timestamp = memory.get("timestamp", "")
            
            memory_lines.append(f"{i+1}. [{importance}] {content} ({timestamp})")
        
        return "\n".join(memory_lines)
    
    def _prepare_messages(self, context: AgentContext) -> List[Dict[str, str]]:
        """
        Prepare the message list for the LLM request.
        
        Args:
            context: The agent context.
        
        Returns:
            List of message dictionaries (role, content).
        """
        messages = []
        
        # Add system prompt
        system_prompt = self._generate_system_prompt(context)
        messages.append({"role": "system", "content": system_prompt})
        
        # Add conversation history
        history_messages = self._format_conversation_history(context)
        messages.extend(history_messages)
        
        # Prepare the user message with the current input
        user_message = context.player_input
        
        # Add memory context if available
        memory_context = self._format_memories(context)
        if memory_context:
            user_message = f"{memory_context}\n\n{user_message}"
        
        # Add context summary if available
        if context.context_summary:
            user_message = f"{context.context_summary}\n\n{user_message}"
        
        # Add the user message
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def process(self, context: AgentContext) -> AgentResponse:
        """
        Process a request with the agent.
        
        Args:
            context: The agent context.
        
        Returns:
            The agent response.
        """
        logger.info(f"Processing request with {self.agent_name} agent")
        
        # Prepare messages for the LLM
        messages = self._prepare_messages(context)
        
        # Get completion from LLM
        try:
            # Ensure we have valid provider and model
            if not self._provider_type:
                logger.warning(f"No provider type set for {self.agent_name} agent. Using default.")
                self._provider_type = self._llm_manager._provider_manager.get_default_provider()
                if not self._provider_type:
                    self._provider_type = ProviderType.OPENAI
            
            # If model is None or empty, use provider default
            effective_model = self._model
            if not effective_model:
                logger.warning(f"No model set for {self.agent_name} agent. Using provider default.")
                effective_model = None  # This will make LLMManager use the provider default
            
            llm_response = self._llm_manager.get_completion(
                messages=messages,
                provider_type=self._provider_type,
                model=effective_model,
                temperature=self._temperature,
                max_tokens=self._settings.get("max_tokens", 1000),
                timeout=self._settings.get("timeout_seconds", 30)
            )
        except Exception as e:
            logger.error(f"Error getting LLM completion: {e}")
            return AgentResponse(
                content=f"Error: {self.agent_name} agent failed to generate a response.\nError details: {str(e)}",
                metadata={"error": "llm_failure", "error_details": str(e)}
            )
        
        if not llm_response:
            logger.error(f"{self.agent_name} agent failed to get LLM response")
            return AgentResponse(
                content=f"Error: {self.agent_name} agent failed to generate a response.",
                metadata={"error": "llm_failure"}
            )
        
        # Process the LLM response
        content = llm_response.content
        
        # Extract any commands from the content
        commands = self._extract_commands(content)
        
        # Create response
        response = AgentResponse(
            content=content,
            commands=commands,
            metadata={
                "agent_name": self.agent_name,
                "agent_id": self.agent_id,
                "provider": llm_response.provider_type.name,
                "model": llm_response.model,
                "tokens": llm_response.total_tokens,
                "cost": llm_response.cost
            },
            llm_response=llm_response
        )
        
        logger.info(f"{self.agent_name} agent generated response ({llm_response.total_tokens} tokens)")
        return response
    
    def _extract_commands(self, text: str) -> List[Tuple[str, str]]:
        """
        Extract LLM commands from text.
        
        Args:
            text: The text to extract commands from.
        
        Returns:
            A list of (command, args) tuples.
        """
        # Use the CommandProcessor's extract method
        from core.base.commands import get_command_processor
        
        command_processor = get_command_processor()
        return command_processor.extract_llm_commands(text)
    
    def reload_settings(self):
        """
        Reload agent settings from configuration file.
        
        This should be called when LLM settings are updated in the UI.
        """
        self._settings = self._load_agent_settings()
        self._provider_type = self._get_provider_type()
        self._model = self._settings.get("model", None)
        self._temperature = self._settings.get("temperature", 0.7)
        
        logger.info(f"Reloaded settings for {self.agent_name} agent")
    
    @abstractmethod
    def supports_command(self, command: str) -> bool:
        """
        Check if this agent supports a specific command.
        
        Args:
            command: The command name.
        
        Returns:
            True if the agent supports the command, False otherwise.
        """
        pass


# Example implementation (for demonstration purposes)
class TestAgent(BaseAgent):
    """
    Test implementation of BaseAgent.
    
    This is a simple implementation for testing and demonstrating the BaseAgent functionality.
    """
    
    def __init__(self):
        """Initialize the test agent."""
        super().__init__("TestAgent")
    
    def _generate_system_prompt(self, context: AgentContext) -> str:
        """Generate system prompt for test agent."""
        return (
            "You are TestAgent, a simple agent for testing the BaseAgent functionality.\n"
            "You should respond briefly and directly to user inputs."
        )
    
    def supports_command(self, command: str) -> bool:
        """Check if the test agent supports a command."""
        # Test agent supports TEST_COMMAND
        return command == "TEST_COMMAND"


# Example usage
if __name__ == "__main__":
    # Set up basic logging
    logging.basicConfig(level=logging.INFO)
    
    # Create a test agent
    test_agent = TestAgent()
    
    # Create a test context
    context = AgentContext(
        game_state={},
        player_state={"name": "Test Player"},
        world_state={"location": "Test Location"},
        player_input="Hello, agent!",
        conversation_history=[
            {"role": "player", "content": "Hello, world!", "timestamp": "2023-01-01T12:00:00"},
            {"role": "gm", "content": "Welcome to the game!", "timestamp": "2023-01-01T12:00:01"}
        ]
    )
    
    # Process the request
    response = test_agent.process(context)
    
    # Print the response
    print(f"Response: {response.content}")
    print(f"Metadata: {response.metadata}")
    
    if response.commands:
        print("Commands:")
        for cmd, args in response.commands:
            print(f"  {cmd}: {args}")
